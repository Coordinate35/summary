| 作者 | 版本号 | 时间 | 内容 |
| :--- | :--- | :--- | :--- |
| Coordinate35 | v1.0.0 | 2025-05-20 | 故障详情与大纲 |

今天在朋友圈偶然看到这个故障，感觉这个故障非常典型，但是公开出来的资料分析感觉可以更进一步。
同时担心以后我曾经做过的事情就忘记了，因此借这个机会记录下来

# 背景

1. Spotify 是瑞典 06 年创建的一家为全球性的音乐服务公司
2. Spotify 使用 Envoy 作为统一接入层网关. 所有公网流量都通过 Envoy 进入内网各个服务中
3. Spotify 为 Envoy 自研了不少 filter， 比如 rate limit filter. 而 Envoy 提供了更加动态的能力: filter 在整个请求的处理顺序是可以通过配置进行调整的。与之相比较, Nginx 的各个模块的处理顺序在编译期确定.

# 时间线

12:18 UTC Spotify 进行了一次配置发布。配置的内容为：调整了 Envoy filter 的执行顺序。由于 Spotify 的同学评估这个变更风险很小，因此直接就全量了。

12:20 UTC 值班人员收到流量突降的报警

12:28 UTC 情况加重：除了亚洲之外所有地区流量掉底。

14:20 UTC 欧洲的流量开始可以进入，但是此时由于端上大量失败重试，流量为正常流量的 2 倍左右。

15:10 UTC US 区域的流量也恢复进入己方。同样的流量非常高

15:40 UTC 流量完全恢复

故障时效性总结：
1. 故障感知耗时: 2 分钟
2. 定位到止损耗时: 120 分钟
3. 故障恢复耗时：80 分钟

# 根因分析

Envoy filter 实现的有问题，调整顺序后会导致其中一个过滤器出错，进而导致 Envoy 进程崩溃

# 故障分析

故障时的流量表现:
![img](_static/traffic.png)

## 为什么流量掉底了这么长时间

Spotify 的同学第一时间就回滚了，问题是回滚之后发现无法恢复。

没有恢复的原因是重试的流量太大，实例一起来就被流量打死了。

crash 掉的实例被 k8s 拉起来后，由于内存占用过高，突破了 k8s 设定的内存限制，导致 k8s 自动关闭实例（可能是故障自愈？）。之后就一直处于此循环往复。

内存占用过高的直接原因定位到是由于 Envoy 的最大堆大小设置的高于允许的内存限制。大量重试流量进来之后，内存马上就到达了上限.

流量的恢复最终是通过扩容完成的。总内存容量的增加降低了单实例的内存需求，最终不再被重复重启

## 为什么亚洲不受影响

从图中可以看出，亚洲的曲线也抖了以下，所以肯定亚洲也受到了配置变更的影响，但是很快就回滚恢复了。

没有受到内存限制的影响是因为时区的原因，亚洲的流量本身比较小


# 感受

1. 影响面不可评估。变更流程的重要性
2. 为什么测试不能发现
3. 容量认识不准确
4. 自愈或是说自动的调节要有阈值保护

# 本次事故设计的稳定性保障方案，我们是怎么做的

## 容量保障

接入曾容量风险的来源
接入曾的容量指标：cpu.idl/cps/磁盘
容量保障的常用手段


## 变更管理

集群分级、划分

二进制、配置

灰度粒度



# 参考资料

1. [!事故报告原文](https://engineering.atspotify.com/2025/05/incident-report-spotify-outage-on-april-16-2025)
2. [!Envoy filter 加载顺序配置](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/http/http_filters)
