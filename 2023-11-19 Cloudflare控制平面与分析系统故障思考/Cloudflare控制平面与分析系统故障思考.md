# 背景

1. Cloudflare 的控制平面和分析系统主要在俄勒冈州希尔斯伯勒附近三个数据中心的服务器上运行。这三个数据中心具备异地多活的能力。
2. 俄勒冈州三个设施中最大的一个由 Flexential 运营。我们将该设施称为“PDX-DC04”
3.  Portland General Electric (PGE)  为 PDX-DC04 提供市电

# 时间线

2023.11.02 08:50 PGE发生了一次计划外维护事件，影响了其大楼的一个独立市电电源。Flexential 执行操作：发电机和备用市电电源共同供电

2023.11.02 11:40 PDX-04 的 PGE 变压器发生接地故障(可能也是 08:50的操作引起)，主备电源同时掉电。

2023.11.02 11:44 接地故障触发了全机房停电预案（包括发电机，为了保护设备），发电机停止工作。cloudflare 侧此时感知到

2023.11.02 12:01 兜底 UPS （可理解成电池）供电 4 分钟（原定是10分钟）用完，服务器掉电。

2023.11.02 12:48 Flexential 重启发电机，但是发现断路器批量损坏（比备用的多）

2023.11.02 13:40 决定向 Cloudflare 位于欧洲的灾难恢复站点进行故障转移

2023.11.02 13:43 开始在灾难恢复站点启动第一批服务（日志除外）。遇到雪崩问题，通过限流，服务逐步恢复。

2023.11.02 17:57 服务大面上基本恢复

2023.11.02 22:48 PDX-DC04恢复供电

2023.11.04 04:25 服务完全恢复

# 关键细节

1. 高可用系统建设不完备：
   1. 设计缺陷：管控系统只有单活，但是多活能力依赖管控系统
   2. 改造不完全：部分强依赖是单活
   3. 测试不充分：没有真正断网演练过
2. 高可用系统腐化：一些较新产品的服务，尚未添加到高可用性集群中。
3. 预警通告不及时：协调世界时 12:28，Flexential 向我们发送了第一条表示他们遇到问题的消息
4. 预案腐化：
	1. UPS 可工作的时间4min << 原定的 10min，同时预案执行的时间也 >> 10min
	2. 机房供电恢复预案中部分场景需要人工执行，现场的夜班人员中没有经验丰富的操作或电气专家——夜班人员包括保安和一名无人陪伴的技术人员，这名技术人员才刚刚上岗一周
	3. 演练不足


# 感受

1. 计划外的东西、和平时不一样的操作，往往是故障的直接原因。事事都考虑非常全面是很难的，因为"全面"是一个很难定义清楚、成模式的一个东西，同时里面也有非常多的惯性的东西干扰。尽量避免比较容易。
2. 对于一个可演进的架构来说，Fitness Function 非常关键：
	1. 迭代过程中是否有破坏架构的行为：新服务没有加入高可用集群
	2. 对于一个产品做到什么程度需要高可用架构，也需要提前定义清楚，并定义到 Fitness Function 中
3. 理想情况下：预案的执行条件可能也需要有系统能联动和管理。毕竟随着系统的复杂度增高，人是比较难把所有的逻辑都考虑到的。毕竟稳定性是一个靠细节堆起来的东西，而无数已知的未知的细节涌上来才是最恶心的。落地形式可能是模版化的设计文档？
4. 跨团队、特别跨公司的联动似乎是永远都做不好的。
 
# 参考链接

1. [!Cloudflare 控制平面和分析服务中断的事后分析](https://blog.cloudflare.com/post-mortem-on-cloudflare-control-plane-and-analytics-outage/)
